---
title: 'Cerebros SLAM'
date: 2019-04-30
permalink: /posts/2019/04/slam/
tags:
  - SLAM
  - Deep Learning
  - ORB-slam
  - Tensorflow
  - KITTI
---

![SLAM](/images/slam.jpg)
# 1. Introduction to SLAM

Implementation of navigation system that uses artificial landmarks or a priori known maps of the environment, and accurate sensor systems to get precise measurements of the landmarks or map features, is straightforward for today’s robots. Similarly, the task of building a map of the environment given the exact position of the robot is largely a solved problem. However, it is much harder to solve the complete problem simultaneously, enabling a mobile robot to build a map of an unexplored environment while simultaneously using this map to localize itself.
Simultaneous Localization and Mapping (SLAM) allows any robot to map its surroundings and localize itself inside that map. The need for SLAM arises because the Inertial measurement units (IMUs) which guide the robot are noisy and may not be available in all environments (GPS is not available indoors). SLAM uses the data from the sensors available and tries to build a map out of the available information in order to aid the navigation of robot. With the availability of cheap cameras, a lot of work has been done with regards to Visual SLAM - using cameras to build the map.
The popularity of the SLAM problem is correlated with the emergence of indoor mobile robotics. The use of GPS has no room to bound the localization error for indoor usage. Additionally, SLAM offers an appealing alternative to user-built maps, showing that robot operation is approachable even in the absence of a purpose specification localization infrastructure.
Important applications of SLAM include:
* Automatic car piloting on unrehearsed off-road terrains
* Rescue tasks for high-risk or difficult-navigation environments
* Planetary, aerial, terrestrial and oceanic exploration
* Augmented reality applications where virtual objects are involved in real-world scenes
* Visual surveillance systems
* Medicine, and many more


![SLAM](/images/SLAM.png "SLAM")

**SLAM**


## 1.1 Place Recognition

The process of identifying the location of a given image by querying the locations of images belonging to the same place in a large geotagged database is known as place recognition. One major characteristic that separates place recognition from other visual recognition tasks is that place recognition has to solve condition invariant recognition to a degree that many other fields haven’t. Place recognition has a number of applications ranging from autonomous driving, robot navigation to augmented reality, geo-localizing archival imagery.
Vision-based localization and mapping systems generally follow this process: images are processed, features are detected, statistical confidence measures are determined and place or image recognition likelihoods are calculated, sometimes in order to generate a map. Images are typically obtained from sensors ranging from video graphics array (VGA) resolution web cameras to top of the line industrial camera. In general, the drive in vision-based mapping and localization research has been towards producing increasingly accurate maps of ever larger environments. For this, place recognition plays a big part.
Several specialized tasks based on recognition exist, such as:
* Content-based image retrieval – finding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative a target image (give me all images similar to image X), or in terms of high-level search criteria given as text input (give me all images which contains many houses, are taken during winter, and have no cars in them).
* Pose estimation – estimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin.
* Optical character recognition (OCR) – identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g. ASCII).
* 2D code reading – reading of 2D codes such as data matrix and QR codes.
* Facial recognition
* Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects

## 1.2 Pose Estimation

In computer vision and robotics, a typical task is to identify specific objects in an image and to determine each object's position and orientation relative to some coordinate system. This information can then be used, for example, to allow a robot to manipulate an object or to avoid moving into the object. The combination of position and orientation is referred to as the pose of an object.
The image data from which the pose of an object is determined can be either a single image, a stereo image pair, or an image sequence where, typically, the camera is moving with a known velocity. The objects which are considered can be rather general, including a living being or body parts, e.g., a head or hands. The methods which are used for determining the pose of an object, however, are usually specific for a class of objects and cannot generally be expected to work well for other types of objects.
The pose can be described by means of a rotation and translation transformation which brings the object from a reference pose to the observed pose. This rotation transformation can be represented in different ways, e.g., as a rotation matrix or a quaternion.
3D pose estimation is the problem of determining the transformation of an object in a 2D image which gives the 3D object. The need for 3D pose estimation arises from the limitations of feature-based pose estimation. There exist environments where it is difficult to extract corners or edges from an image. To circumvent these issues, the object is dealt with as a whole through the use of free-form contours.


![Pose Estimation](/images/Pose Estimation.png "Pose Estimation")

**Pose Estimation**


### 1.2.1 Quaternions

Quaternions are a number system that extends the complex numbers. A feature of quaternions is that multiplication of two quaternions is noncommutative. It is defined as the quotient of two directed lines in a three-dimensional space or equivalently as the quotient of two vectors.
Unit quaternions, also known as versors, provide a convenient mathematical notation for representing orientations and rotations of objects in three dimensions. Compared to Euler angles they are simpler to compose and avoid the problem of gimbal lock. Compared to rotation matrices they are more compact, more numerically stable, and more efficient. When used to represent rotation, unit quaternions are also called rotation quaternions as they represent the 3D rotation group. When used to represent an orientation (rotation relative to a reference coordinate system), they are called orientation quaternions or attitude quaternions. The equation for spatial rotations can be summarized for T radians about a unit axis (X,Y,Z) as the Quaternion (C, XS, YS, Z*S) where $$ C=cos(T/2) $$ and $$ S=sin(T/2) $$.
Quaternions have 4 dimensions (each quaternion consists of 4 scalar numbers), one real dimension and 3 imaginary dimensions. Each of these imaginary dimensions has a unit value of the square root of -1, but they are different square roots of -1 all mutually perpendicular to each other, known as i,j and k. So a quaternion can be represented as follows:
> $$ a + i b + j c + k d $$

There are 3 square roots of -1 and they are working in 4 dimensions so there are at least 3 ways to get round from +1 to -1. It is not very practical to try to draw 4 dimensions in 2 dimensions, but it is shown as follows:


![Quaternions](/images/Quaternion.png "Quaternions")

**Quaternions**


The main practical application of this interesting algebra is to represent 3D rotations.
In fact quaternions can represent 3D reflections, rotations and scaling, however a single quaternion operation cannot include translations so if we want to rotate, reflect or scale around a point other than the origin, then we would have to handle the translation part separately. To calculate the resulting point ($$P_{out}$$) when we translate the point ($$P_{in}$$) using quaternions then we use the following equations:
>
* For Reflection & scaling: $$ P_{out} = q P_{in} q $$
* For Rotation & scaling: $$ P_{out} = q P_{in} conj(q) $$

The majority of applications involve pure rotations, for this we restrict the quaternions to those with unit magnitude and we use only multiplications and not addition to represent a combination of different rotations. When quaternions are normalised in this way, together with the multiplication operation to combine rotations, form a mathematical group. We can use this to do lots of operations which are required in practical applications such as, combining subsequent rotations (and equivalently orientations), interpolating between them, etc. When quaternions are used in this way we can think of them as being similar to axis-angle except that real part is equal to $$ cos(T/2) $$ and the complex part is made up of the axis vector times $$ sin(T/2) $$.


# 2 Classical SLAM

[ORB SLAM](http://webdiis.unizar.es/~raulmur/orbslam/) has been the benchmark for classical SLAM for a long time now. There have been various improvements on top of the ORB SLAM. However, all of them use the same pipeline and the improvements in monocular SLAM haven't been significant. Check the leaderboard on [KITTI dataset](http://www.cvlibs.net/datasets/kitti/eval_odometry.php).
ORB SLAM has the following pipeline:
1. Pose recognition
* Identify a set of points to track in the current frame.
* Track the points in the next frame
* Perform 2D-3D translation of the points using the tracking from frame 1 to frame 2.
* Repeat the steps for every frame.
2. Map Building.
* Using the 2D-3D translation from pose recognition, associate every point to a frame.
* Add the points to the map.
* Insert keyframe of poses. 2 keyframes have significant difference between the set of points visible between them.
* The Map consists of a graph of keyframes and the 3D points visible from each of the KeyFrame.
* KeyFrames are connected if points visible from one of the KeyFrame is also visible in the other KeyFrame.
3. Place Recognition.
* Describe the current image using the pre-defined vocabulary of descriptors.
* Keep building a bag of words for every frame.
* Check if the current frames bag matches any other frame's bag. If yes, initiate loop closure optimizations.
4. Loop Closure Optimizations
* Runs when a loop has been identified.
* It adds constraints to the existing Map and poses based on the identification of loop and performs non linear optimizations based on the constraints to correct the error.

As seen above, the entire pipeline uses classical Computer Vision techniques and understanding to solve the problem. The pipeline is also very elaborate to get considerably accurate result.

## 2.1 Results
The challenges with monocular SLAM is that it is difficult to get scale with a single camera. If you look at the leaderboard with KITTI dataset, the ones which perform better are mostly either RGB-D datasets, stereo datasets, or LIDAR based SLAM. All of them have an inherent advantage of knowing the depth beforehand through an active sensor. The classical SLAM has to make an assumption of depth and hope that it is able to correct it in the future with its loop closure optimization, if there is a loop in the sequence.
ORB-SLAM overall has an error percent of 1.15 % in translation and 0.0027 [deg/m] in rotation. That is in real life, the Algorithm is able to correctly estimate the error in translation within 1.15 meters of a 100 meter displacement. While that may sound a lot over large distances, the accuracy by itself is an achievement.

## 2.2 Datasets

There have been various datasets other than KITTI on which ORB-SLAM is tested on, which includes:
>
1. [TUM](https://vision.in.tum.de/data/datasets/mono-dataset)
2. [EuroCC](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets)

We are trying to tackle the components of Place Recognition and Pose Recognition using Deep Learning. Thw work done by us is explained in [Section 3](#Place) and [Section 4](#Pose) below

# <a name="Place">3 Place Recognition</a>

Our approach is based on the architecture proposed in paper [NetVLAD: CNN architecture for weakly supervised place recognition](https://arxiv.org/pdf/1511.07247.pdf). The NetVLAD-CNN will transform an image into a compact single Vector Representation amenable to effective indexing. Based on the Euclidean distance of the Vectors, similar frames can be identified, which can be used to detect and optimize the loop closure.

## 3.1 Architecture

The architecture consists of two steps 1. Extraction of the Local Descriptor, and 2. Pool the local descriptors in an orderless manner to generate a Vector Representation. Here, VGG16 is used to extract its descriptors and the output is a  H x W x D map which can be considered as a set of D-dimension descriptors extracted at H x W spatial locations. Later, a new pooling layer Vector of Locally Aggregated Descriptors (VLAD) is added to the convolution, as shown in the Figure. VLAD takes N D-dimensional local image descriptors $x_{i}$ as input and $K$ cluster centers $c_{k}$ as VLAD parameter, and the outputs an Image Vector Representation V of K x D-dimensions.
>
$$V(j,k)=\sum_{i=1}^{N} \frac{e^{w_{k}^{T}x_{i}+b_{k}}}{\sum_{k'}^{N} e^{w_{k}^{T}x_{i}+b_{k}}} (x_{i}(j)-c_{k}(j))$$

where $$w_{k}$$, $$b_{k}$$ and $$c_{k}$$ are sets of trainable parameters for each cluster $$k$$.


![Architecture](/images/PlaceRecogArchitecture.JPG "Architecture")

**Architecture**


## 3.2 How it Works?

A weakly supervised triplet $$(q, P_{i}, N_{j})$$ approach is used for the training. A triplet consists of a query image $$q$$, a set of potential positives $$P_{i}$$ (images close to the query based on GPS information) and definite negatives $$N_{j}$$ (images geographically far from the query). The Euclidean distance for the potential positives $$d(q, P_{i})$$ and definite negatives $$d(q, N_{j})$$ should to be maximized and minimized respectively. Based on this intuition, the weakly supervised ranking loss $$L_{θ}$$ for a training tuple $$(q, P_{i}, N_{j})$$ along with hinge loss $$l$$ is calculated as:

$$L_{\Theta}=\sum_{j}l(min_{i}  d_{\Theta}^{2}(q,p_{i}) + m - d_{\Theta}^{2}(q,n_{j}))$$


## 3.3 Results

The figure shows the result of the NetVLAD-CNN on Kitti Dataset. The confusion matrix computes the distance of all the frames at time i and j. The Confusion matrix GPS depicts the actual distance between the frames based on the GPS location, and the Confusion matrix NetVLAD shows the result of NetVLAD-CNN. The dark blue color represents similar frames and our model is able to identify the same.


![Results](/images/PlaceRecogResults.JPG "Results")

**Results**

# <a name="Pose">4. Pose Net </a>
We worked on the architecture proposed in the paper [Relative Camera Pose Estimation Using CNN]( https://arxiv.org/abs/1702.01381). The relative camera pose is $$\Delta P$$ can be divided into two separate components i.e Translation ($$\Delta T$$) and Rotation ($$\Delta R$$). The Translation is a relative position vector $$\Delta T$$ (3-dimensional quantity) and  Rotation is relative orientation vector $$\Delta R$$ (4-dimensional quaternion).

## 4.1 Architecture

The model involves extracting features from the images using convolutional layers and followed by fully-connected layers. The network is obtained by removing the last three fully-connected layers layers (FC6, FC7 and FC8) from the original Hybrid-CNN preserving only convolutional, max-pooling layers
and ReLU. The Hybrid-CNN is a AlexNet fine tuned on Places Dataset. The architecture is shown below. Both the branches share weights.


![Architecture](/images/pose_net_arch.png "Pose Net Architecture")

**Architecture**


## 4.2 How does it work?
The output of the final convolutional layer is of the shape 13x13x128. We pass two frames through the network and obtain their representations.

Then we merge both the representations and get 13x13x256 vector. The regression part in turn is composed of two fully-connected (FC1 and FC2) layers, where FC1 and FC2 have 4 and 3 connections respectively for estimating the 7-dimensional pose vector.

The loss function is given below:
>
$$ L  =   |  \Delta  T - \Delta \hat{T}   |   + \beta *    |  \Delta Q - \Delta \hat{Q} | $$

## 4.3 Results


![Ground Truth](/images/target.jpg "Ground Truth")

**Ground Truth**

![Output](/images/output.jpg "Output")

**Output**


### Root Mean Square error

  | Value | Error |
 | --- | --- |
 | Tx | 0.78792044 |
 | Ty | 0.04065268 |
 | Tz | 0.08842828 |
 | Qw | 0.01129969 |
 | Qx | 0.06170158 |
 | Qy | 0.37319901 |
 | Qz | 0.48971555 |
 |---|---|


# Contributions:
* Siddharth Agrawal
    * Working with Classical SLAM. Understanding of various components and datasets.Implementing and testing Place Recognition.
* Muthu Palaniappan
  * Literature Survey for Pose Recognition. Implementing Pose Recognition. Testing Pose Recognition.
* Akarsh Goyal
  * Implementing Pose Recognition. Testing Pose Recognition. Testing translation from Tensorflow to C++ for integration.
* Abhay Atrish
  * Literature Survey for Place Recognition. Implementing Place Recognition. Testing Place Recognition.
